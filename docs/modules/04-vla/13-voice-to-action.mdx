---
title : "13. Voice-to-Action with Whisper & Gemini"
sidebar_label : "13. Voice-to-Action"
---

## Chapter 13 : Voice-to-Action with Whisper + Gemini 2.5 Flash

### The Final Frontier : Natural Language Interaction

We have reached the culmination of our journey. We have built a humanoid robot, simulated it with photorealistic and physically-accurate engines, and programmed it to walk and manipulate objects. Now, we will give it the ability to *understand* and *act* upon human speech. This is the "holy grail" of human-robot interaction : a true Voice-Language-Action (VLA) pipeline.

In this chapter, we will build a complete, end-to-end system that allows you to speak a command, and watch the robot execute it in simulation. Our pipeline will be :
1.  **Listen :** A Python script will capture live audio from your microphone using Voice Activity Detection (VAD).
2.  **Transcribe :** The audio will be fed into a local, offline instance of OpenAI's **Whisper** model to be converted into text.
3.  **Understand :** The transcribed text will be sent to **Google's Gemini 2.5 Flash** model with a carefully engineered prompt, which will parse the user's intent into a structured JSON command.
4.  **Act :** A ROS 2 node will receive this JSON command and trigger the appropriate ROS 2 Action Server (e.g., navigation or manipulation) to make the robot perform the task.

Our final demonstration will be simple, yet profound : you will say "Bring me the red cup," and the humanoid robot in Isaac Sim will begin moving towards the red cup.

<Mermaid chart={`
graph TD;
    A[Microphone] -- "Raw Audio" --> B(Whisper);
    B -- "Transcribed Text" --> C{Gemini 2.5 Flash};
    C -- "Structured JSON" --> D["ROS 2 Action Client"];
    D -- "Action Goal" --> E["ROS 2 Action Server<br/>(e.g., MoveIt 2)"];
    E -- "Joint Commands" --> F["Simulated Robot"];

    subgraph "Your Local Machine"
        A
        B
        C
        D
    end
    subgraph "Robot Control System"
        E
    end
    subgraph "Isaac Sim"
        F
    end

    style C fill :#00ff9d
`} />

### The VLA Pipeline : A ROS 2 Implementation

We will create a new ROS 2 package called `voice_to_action` containing four distinct nodes. This modularity is key to building a robust and debuggable system.

<Mermaid chart={`
graph TD;
    subgraph "voice_to_action ROS 2 Package"
        A[mic_publisher_node] -- "/voice_audio" --> B[speech_to_text_node];
        B -- "/transcribed_text" --> C[nlu_node];
        C -- "/robot_action_goal" --> D[action_client_node];
    end
    style A fill :#lightblue
    style B fill :#lightblue
    style C fill :#lightblue
    style D fill :#lightblue
`} />

### Step 1 : Capturing Voice (`mic_publisher_node`)

This node's only job is to listen to the microphone and publish audio chunks when it detects speech. We'll use the `sounddevice` library for audio capture and a simple energy-based VAD.

**`mic_publisher_node.py` :**
```python
import rclpy
from rclpy.node import Node
from std_msgs.msg import Int16MultiArray
import numpy as np
import sounddevice as sd

class MicPublisherNode(Node) :
    def __init__(self) :
        super().__init__('mic_publisher_node')
        self.publisher = self.create_publisher(Int16MultiArray, '/voice_audio', 10)
        self.samplerate = 16000
        self.silence_threshold = 300  # Energy threshold for silence
        self.silence_duration = 1.5  # Seconds of silence to trigger publish
        self.get_logger().info("Listening...")

    def listen_and_publish(self) :
        recorded_frames = []
        silent_frames = 0
        is_speaking = False

        def audio_callback(indata, frames, time, status) :
            nonlocal recorded_frames, silent_frames, is_speaking
            energy = np.linalg.norm(indata)
            if energy > self.silence_threshold :
                is_speaking = True
                silent_frames = 0
                recorded_frames.append(indata.copy())
            elif is_speaking :
                silent_frames += 1
                recorded_frames.append(indata.copy())
                if (silent_frames * (frames / self.samplerate)) > self.silence_duration :
                    self.get_logger().info("Silence detected, publishing audio.")
                    audio_data = np.concatenate(recorded_frames, axis=0)
                    msg = Int16MultiArray()
                    # Assuming 16-bit audio, scale and convert
                    msg.data = (audio_data * 32767).astype(np.int16).tolist()
                    self.publisher.publish(msg)
                    
                    # Reset
                    recorded_frames.clear()
                    is_speaking = False
                    silent_frames = 0
                    self.get_logger().info("Listening...")

        with sd.InputStream(callback=audio_callback, channels=1, samplerate=self.samplerate, dtype='float32') :
            rclpy.spin(self)

def main(args=None) :
    rclpy.init(args=args)
    node = MicPublisherNode()
    node.listen_and_publish()
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__' :
    main()

    import rclpy
from rclpy.node import Node
from std_msgs.msg import String as StringMsg
from std_msgs.msg import Int16MultiArray
import numpy as np
import whisper # Requires 'pip install openai-whisper'

class SpeechToTextNode(Node) :
    def __init__(self) :
        super().__init__('speech_to_text_node')
        self.subscription = self.create_subscription(
            Int16MultiArray, '/voice_audio', self.listener_callback, 10)
        self.publisher = self.create_publisher(StringMsg, '/transcribed_text', 10)
        
        self.get_logger().info("Loading Whisper model...")
        self.model = whisper.load_model("base.en") # Using the small English model for speed
        self.get_logger().info("Whisper model loaded.")

    def listener_callback(self, msg) :
        self.get_logger().info("Received audio for transcription.")
        # Convert Int16MultiArray back to a NumPy array
        audio_data = np.array(msg.data, dtype=np.int16).astype(np.float32) / 32767.0
        
        result = self.model.transcribe(audio_data)
        transcribed_text = result['text']
        
        self.get_logger().info(f'Whisper transcribed : "{transcribed_text}"')
        
        # Publish the text
        text_msg = StringMsg()
        text_msg.data = transcribed_text
        self.publisher.publish(text_msg)

def main(args=None) :
    rclpy.init(args=args)
    node = SpeechToTextNode()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__' :
    main()

    import rclpy
from rclpy.node import Node
from std_msgs.msg import String as StringMsg
import google.generativeai as genai
import json
import os

# Configure with your API key
genai.configure(api_key=os.environ[GEMINI_API_KEY])

class NluNode(Node) :
    def __init__(self) :
        super().__init__('nlu_node')
        self.subscription = self.create_subscription(
            StringMsg, '/transcribed_text', self.listener_callback, 10)
        self.publisher = self.create_publisher(StringMsg, '/robot_action_goal', 10)
        
        self.get_logger().info("Initializing Gemini model...")
        self.model = genai.GenerativeModel('gemini-1.5-flash') # Using Gemini 1.5 Flash
        self.get_logger().info("NLU node ready.")
        
    def build_prompt(self, user_text) :
        # This prompt is the most critical part of the system.
        # It constrains the LLM to a specific output format and set of known actions/objects.
        return f"
        You are the Natural Language Understanding (NLU) unit for a household robot.
        Your task is to convert a user's spoken command into a structured JSON object.
        The robot has a limited set of capabilities and known objects.

        # Allowed Actions :
        - "GO_TO" : Navigate to a named location.
        - "PICK_UP" : Pick up a specific object.

        # Known Objects & Locations :
        - "red_cup"
        - "blue_block"
        - "kitchen_table"
        - "charging_station"

        # User Command :
        "{user_text}"

        # Output :
        Convert the user command into a single JSON object with two keys : "action" and "target".
        The "action" must be one of the Allowed Actions.
        The "target" must be one of the Known Objects or Locations.
        If the command is unclear or requests something impossible, output a JSON object with the action "FAIL".
        Respond with ONLY the JSON object and nothing else.
        "

    def listener_callback(self, msg) :
        user_command = msg.data
        self.get_logger().info(f'Received text : "{user_command}"')
        
        prompt = self.build_prompt(user_command)
        
        try :
            response = self.model.generate_content(prompt)
            # Clean up the response to get only the JSON
            json_text = response.text.strip().replace("`", ").replace("json", ")
            self.get_logger().info(f"Gemini responded with : {json_text}")

            # Validate and publish
            parsed_json = json.loads(json_text)
            if "action" in parsed_json and "target" in parsed_json :
                json_msg = StringMsg()
                json_msg.data = json.dumps(parsed_json)
                self.publisher.publish(json_msg)
            else :
                 self.get_logger().error("Invalid JSON from LLM.")

        except Exception as e :
            self.get_logger().error(f"Failed to call Gemini or parse response : {e}")

def main(args=None) :
    rclpy.init(args=args)
    node = NluNode()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__' :
    main()

    import rclpy
from rclpy.node import Node
from rclpy.action import ActionClient
from std_msgs.msg import String as StringMsg
from my_robot_interfaces.action import PickUpObject, NavigateToPose # Assumed custom actions
from geometry_msgs.msg import PoseStamped
import json

class ActionClientNode(Node) :
    def __init__(self) :
        super().__init__('action_client_node')
        self.subscription = self.create_subscription(
            StringMsg, '/robot_action_goal', self.listener_callback, 10)
        
        # Action clients to the robot's capabilities
        self._pickup_client = ActionClient(self, PickUpObject, '/pickup_object')
        self._navigate_client = ActionClient(self, NavigateToPose, '/navigate_to_pose')

        # Pre-defined knowledge of the world
        self.object_locations = {
            "red_cup" : PoseStamped(), # Fill in actual coordinates
            "blue_block" : PoseStamped()
        }
        self.get_logger().info("Action client node ready.")

    def listener_callback(self, msg) :
        try :
            command = json.loads(msg.data)
            action_type = command.get("action")
            target = command.get("target")
            self.get_logger().info(f"Received action '{action_type}' for target '{target}'")

            if action_type == "PICK_UP" :
                if target in self.object_locations :
                    goal_pose = self.object_locations[target]
                    self.send_pickup_goal(goal_pose)
                else :
                    self.get_logger().error(f"Unknown target for PICK_UP : {target}")
            
            elif action_type == "GO_TO" :
                 # Similar logic for navigation...
                 pass

        except json.JSONDecodeError :
            self.get_logger().error("Received invalid JSON action goal.")

    def send_pickup_goal(self, pose) :
        self._pickup_client.wait_for_server()
        goal_msg = PickUpObject.Goal()
        goal_msg.target_pose = pose
        self._pickup_client.send_goal_async(goal_msg)
        self.get_logger().info(f"Sent PICK_UP goal to action server.")

def main(args=None) :
    rclpy.init(args=args)
    node = ActionClientNode()
    rclpy.spin(node)
    node.destroy_node()
    rclpy.shutdown()

if __name__ == '__main__' :
    main()