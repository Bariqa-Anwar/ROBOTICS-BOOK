---
title: "Module 4 – Vision-Language-Action (VLA) & Conversational Robotics"
sidebar_label: Overview
sidebar_position: 0
---

**Weeks 11–13 + Capstone** • **12,000–15,000 words** • **The grand finale**

### Why this module exists
The ultimate test of Physical AI: can your humanoid understand spoken natural language and act on it in the real world?

### Learning Outcomes
- Turn voice commands into executable ROS 2 action sequences in <200 ms
- Build end-to-end offline pipelines (Whisper → Gemini 2.5 Flash → ROS 2)
- Complete the capstone: a fully autonomous conversational humanoid

### Chapters in this module
1. **Chapter 12** – Vision-Language-Action Models: The New Paradigm  
2. **Chapter 13** – Voice-to-Action with Whisper + Gemini 2.5 Flash  
3. **Chapter 14** – LLM-to-ROS 2 Translation Layer  
4. **Chapter 15** – Capstone Project: “Clean the room” from a single spoken command

### Final Capstone Demo (must work in simulation AND on real Jetson + robot)
You say: **“Bring me the red cup from the kitchen”**  
→ The humanoid wakes up, navigates, finds the cup, grasps it, and returns—all autonomously.

<Highlight>This is the moment the book (and the course) proves Physical AI has arrived.</Highlight>
