---
title: "14. The LLM-to-ROS 2 Translation Layer"
sidebar_label: "14. LLM-to-ROS 2 Translation"
---

import { Mermaid } from 'mdx-mermaid';

## Chapter 14: The LLM-to-ROS 2 Translation Layer

### From Command to Choreography

In the previous chapter, we built a remarkable Voice-to-Action pipeline. It could understand a simple, direct command and map it to a single robot action. But what happens when the command is complex? What if we say, "Clean the kitchen," "Inspect the gear assembly," or "Prepare for visitor arrival"? These are not single actions; they are multi-step tasks, chores, choreographies.

A simple one-to-one mapping from language to action is no longer sufficient. We need a layer of reasoning, a task planner that can decompose a high-level goal into a sequence of concrete, executable steps. This chapter is dedicated to building that layer.

We will create a sophisticated **LLM-to-ROS 2 Translation Layer**. This system will use the powerful "function calling" or "tool use" capabilities of models like Gemini 2.5 Flash to act as a high-level task planner for our robot. We will give the LLM a "toolbox" representing the robot's skills and ask it to generate a step-by-step plan for any given command. We will then execute this plan, with robust error handling and the ability to ask the LLM to re-plan when things go wrong.

<Mermaid chart={`
graph TD;
    A["User Command<br/><i>'Clean the room'</i>"] --> B{Translation Layer};
    subgraph B
        B1[LLM Function Calling]
        B2[ROS 2 Action Dispatcher]
        B3[Error Handling & Re-planning]
    end
    B --> C["Action Sequence<br/><i>1. navigate_to('table')<br/>2. pick_up('cup')<br/>3. ...</i>"];
    style B fill:#00ff9d
`} />

### The Function Calling Paradigm

Modern LLMs can do more than just generate text. We can provide them with a list of available "tools" (functions) and their descriptions. When given a prompt, the LLM can respond with a structured JSON object requesting to call one or more of these tools with specific arguments.

This is the perfect mechanism for our translation layer. We can represent each of our robot's core capabilities (each ROS 2 Action Server) as a "tool" for the LLM.

<Mermaid chart={`
sequenceDiagram
    participant User
    participant Translator
    participant LLM
    participant Robot
    User->>Translator: "Tidy up the desk"
    Translator->>LLM: Prompt + Tool Schemas (navigate, pickup, place)
    LLM-->>Translator: Plan: [pickup("cup"), place("cup", "bin")]
    Translator->>Robot: Execute: pickup("cup")
    Robot-->>Translator: Success
    Translator->>Robot: Execute: place("cup", "bin")
    Robot-->>Translator: Success
    Translator-->>User: Task Complete
`} />

### Step 1: Defining the Robot's "API" as Python Functions

First, we create simple Python function stubs inside our new ROS 2 node. Each function represents a high-level skill the robot possesses. The function body itself doesn't matter; its signature, type hints, and docstring are what we care about.

**`llm_action_translator_node.py` (Initial Tools):**
```python
from typing import Literal

# Define known locations and objects for type hinting
KnownLocations = Literal["kitchen_table", "coffee_table", "charging_station", "trash_bin"]
KnownObjects = Literal["red_cup", "blue_block"]

def navigate_to(location: KnownLocations):
    """
    Moves the robot base to a specified, named location in the environment.

    Args:
        location: The destination for the robot's base.
    """
    pass

def pick_up_object(object_name: KnownObjects):
    """
    Executes a grasp sequence to pick up a specified object. Assumes the robot
    is already near the object.

    Args:
        object_name: The name of the object to pick up.
    """
    pass

def place_object(location: KnownLocations):
    """
    Places the currently held object at the specified location. Assumes the
    robot is already at the destination.

    Args:
        location: The location to place the object.
    """
    pass
```

### Step 2: Generating Schemas for the LLM

Now, we need to convert these Python functions into a JSON Schema format that the LLM can understand. This is where the `function_schema` utility from the provided code snippets becomes invaluable. It uses Python's `inspect` and `pydantic` libraries to automatically generate a detailed schema from the function's signature and docstring.

**Using `function_schema`:**
```python
# In our translator node's __init__ method...

# List of functions we want to expose to the LLM
self.robot_tools = [navigate_to, pick_up_object, place_object]

# Generate schemas for each tool
self.tool_schemas = [function_schema(tool) for tool in self.robot_tools]

# We will later convert these schemas to the format required by the Gemini API
self.gemini_tools = [convert_schema_to_gemini_tool(s) for s in self.tool_schemas]
```

The generated schema for `pick_up_object` would look something like this:
```json
{
  "name": "pick_up_object",
  "description": "Executes a grasp sequence to pick up a specified object...",
  "parameters": {
    "type": "object",
    "properties": {
      "object_name": {
        "type": "string",
        "description": "The name of the object to pick up.",
        "enum": ["red_cup", "blue_block"]
      }
    },
    "required": ["object_name"]
  }
}
```
This tells the LLM everything it needs to know to call the function correctly.

### Step 3: The Translator Node - Prompting and Parsing

The core of our system is a ROS 2 Action Server that accepts a natural language command. When it receives a command, it constructs a prompt and calls the Gemini API.

**`llm_action_translator_node.py` (Core Logic):**
```python
import rclpy
from rclpy.node import Node
from rclpy.action import ActionServer
# ... other imports
import google.generativeai as genai

class LLMActionTranslatorNode(Node):
    def __init__(self):
        super().__init__('llm_action_translator_node')
        # ... (tool schema generation from Step 2) ...

        # The Action Server that receives natural language commands
        self._action_server = ActionServer(
            self,
            ExecuteNaturalLanguageTask, # A custom action type
            '/execute_natural_language_task',
            self.execute_callback
        )
        
        # Setup Gemini Model
        self.model = genai.GenerativeModel('gemini-1.5-flash', tools=self.gemini_tools)
        
        # ROS 2 Action Clients for the robot's low-level skills
        self._nav_client = ActionClient(self, NavigateToPose, '/navigate_to_pose')
        self._pickup_client = ActionClient(self, PickUpObject, '/pickup_object')
        # ... etc.

    def execute_callback(self, goal_handle):
        user_command = goal_handle.request.command
        self.get_logger().info(f"Received task: '{user_command}'")
        
        # Initial call to the LLM to get the first plan
        response = self.model.generate_content(user_command)
        
        # The response will contain a list of function calls
        plan = response.candidates[0].content.parts
        
        # Now, we execute the plan...
        # ... (see Step 4)
```

### Step 4: The Execution Engine and Reasoning Loop

This is the most critical part. We don't just execute the plan; we create a loop that can handle failures and ask the LLM to re-plan.

<Mermaid chart={`
graph TD
    A[Get LLM Plan] --> B{For each step in Plan};
    B -- Execute Step --> C[Call ROS Action Server];
    C --> D{Success?};
    D -- Yes --> B;
    D -- No --> E[Report Failure to LLM];
    E --> F[Get New Plan from LLM];
    F --> A;
    B -- All Steps Done --> G[Task Complete];
    style G fill:#ccffcc
    style E fill:#ffcccc
`} />

**`llm_action_translator_node.py` (Execution Loop):**
```python
# ... inside the execute_callback method

conversation_history = [
    {'role': 'user', 'parts': [{'text': user_command}]},
    response.candidates[0].content 
]

while True:
    # Check if the last response contained function calls to execute
    if not conversation_history[-1].parts or not conversation_history[-1].parts[0].function_call:
        self.get_logger().info("LLM finished planning.")
        break # Exit loop if LLM has no more actions

    # Execute the function calls proposed by the LLM
    proposed_plan = conversation_history[-1].parts
    tool_results = []

    for tool_call in proposed_plan:
        func_name = tool_call.function_call.name
        args = tool_call.function_call.args
        self.get_logger().info(f"Executing: {func_name}({args})")
        
        # --- Dispatcher Logic ---
        success = False
        error_message = ""
        if func_name == "navigate_to":
            success, error_message = self.execute_navigation(args['location'])
        elif func_name == "pick_up_object":
            success, error_message = self.execute_pickup(args['object_name'])
        # ... etc.

        # Append the result of the tool execution for the next LLM call
        tool_results.append({
            "function_call": tool_call.function_call,
            "function_response": {
                "name": func_name,
                "response": {"success": success, "error": error_message}
            }
        })
        
        # If any step fails, we stop executing this plan and re-plan
        if not success:
            break

    # --- Re-planning Logic ---
    # Send the results of our execution back to the LLM
    conversation_history.append({'role': 'user', 'parts': tool_results})
    response = self.model.generate_content(conversation_history)
    conversation_history.append(response.candidates[0].content)

# Once the loop finishes, the task is done
goal_handle.succeed()
result = ExecuteNaturalLanguageTask.Result()
result.status = "Task completed successfully."
return result

# --- Helper execution methods ---
def execute_navigation(self, location):
    # 1. Look up coordinates for 'location'
    # 2. Create NavigateToPose.Goal message
    # 3. Call self._nav_client.send_goal_and_wait_for_result()
    # 4. Return (success_boolean, error_string)
    pass
```

### The Demonstration: "Clean the room"

With the full system in place, we can finally tackle a complex, ambiguous command.

1.  **Set up the Scene:** In Isaac Sim, load the apartment world from Chapter 6. Place a red cup model at a known location (e.g., on a table). Place the humanoid robot at a starting position.
2.  **Define Action Servers:** Ensure the robot's control system is running, with Action Servers for `/pickup_object` and `/navigate_to_pose` available (as developed in previous chapters). You will need to fill in the `object_locations` dictionary in `action_client_node.py` with the correct coordinates of the red cup in your scene.
3.  **Launch the VLA Pipeline:** Create a launch file, `voice_to_action.launch.py`, that starts all four of our new nodes.
4.  **Speak the Command:** Run the launch file. The `mic_publisher_node` will print "Listening...". Speak clearly into your microphone: **"Bring me the red cup."**
5.  **Observe the Chain Reaction:**
    *   The mic node will detect silence and publish the audio.
    *   The Whisper node will log: `Whisper transcribed: "Bring me the red cup."`
    *   The NLU node will log: `Gemini responded: {"action": "PICK_UP", "target": "red_cup"}`
    *   The action client node will log: `Sent PICK_UP goal to action server.`
6.  **Watch the Robot Move:** In the Isaac Sim window, you will see the humanoid robot begin to execute its pre-programmed pickup behavior, moving its arm towards the red cup.

### Conclusion

The translation layer we've built is a monumental step towards truly intelligent robots. We have moved beyond simple command-and-control and created a system where the LLM acts as a **symbolic task planner**. It reasons about a high-level goal and decomposes it into a sequence of actions that are grounded in the robot's physical capabilities.

By integrating function calling with a robust ROS 2 action execution and re-planning loop, we have built a system that is not only powerful but also resilient to failure. This architecture is the blueprint for creating robots that can handle the complexity and ambiguity of real-world human instructions, paving the way for a future where interacting with a robot is as natural as talking to a person.

***
_(This file provides the complete code and conceptual framework for a full VLA pipeline. To run it, the user must create a ROS 2 package from the provided Python nodes, install the dependencies like `whisper` and `google-generativeai`, and have pre-existing ROS 2 Action Servers for their robot's physical capabilities.)_
